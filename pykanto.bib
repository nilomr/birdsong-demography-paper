@misc{2021a,
  title = {Audacity({{R}}): {{Free Audio Editor}} and {{Recorder}}},
  author = {, Audacity Team},
  year = {2021}
}

@misc{bechtold2022,
  title = {Soundfile},
  author = {Bechtold, Bastian and Geier, Matthias},
  year = {2022}
}

@article{bergler2019,
  title = {{{ORCA-SPOT}}: {{An Automatic Killer Whale Sound Detection Toolkit Using Deep Learning}}},
  shorttitle = {{{ORCA-SPOT}}},
  author = {Bergler, Christian and Schr{\"o}ter, Hendrik and Cheng, Rachael Xi and Barth, Volker and Weber, Michael and N{\"o}th, Elmar and Hofer, Heribert and Maier, Andreas},
  year = {2019},
  month = jul,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {10997},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-47335-w},
  abstract = {Large bioacoustic archives of wild animals are an important source to identify reappearing communication patterns, which can then be related to recurring behavioral patterns to advance the current understanding of intra-specific communication of non-human animals. A main challenge remains that most large-scale bioacoustic archives contain only a small percentage of animal vocalizations and a large amount of environmental noise, which makes it extremely difficult to manually retrieve sufficient vocalizations for further analysis \textendash{} particularly important for species with advanced social systems and complex vocalizations. In this study deep neural networks were trained on 11,509 killer whale (Orcinus orca) signals and 34,848 noise segments. The resulting toolkit ORCA-SPOT was tested on a large-scale bioacoustic repository \textendash{} the Orchive \textendash{} comprising roughly 19,000 hours of killer whale underwater recordings. An automated segmentation of the entire Orchive recordings (about 2.2 years) took approximately 8 days. It achieved a time-based precision or positive-predictive-value (PPV) of 93.2\% and an area-under-the-curve (AUC) of 0.9523. This approach enables an automated annotation procedure of large bioacoustics databases to extract killer whale sounds, which are essential for subsequent identification of significant communication patterns. The code will be publicly available in October 2019 to support the application of deep learning to bioaoucstic research. ORCA-SPOT can be adapted to other animal species.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Animal behaviour,Behavioural ecology},
  file = {/home/nilomr/Zotero/storage/VCG3HBC7/Bergler et al. - 2019 - ORCA-SPOT An Automatic Killer Whale Sound Detecti.pdf;/home/nilomr/Zotero/storage/ASY2KHA8/s41598-019-47335-w.html}
}

@misc{boersma2022,
  title = {Praat: Doing Phonetics by Computer},
  author = {Boersma, Paul and Weenink, David},
  year = {2022}
}

@manual{bokeh2018,
  type = {Manual},
  title = {Bokeh: {{Python}} Library for Interactive Visualization},
  author = {{Bokeh Development Team}},
  year = {2018}
}

@inproceedings{cannam2010,
  title = {Sonic Visualiser: An Open Source Application for Viewing, Analysing, and Annotating Music Audio Files},
  shorttitle = {Sonic Visualiser},
  booktitle = {Proceedings of the International Conference on {{Multimedia}} - {{MM}} '10},
  author = {Cannam, Chris and Landone, Christian and Sandler, Mark},
  year = {2010},
  pages = {1467},
  publisher = {{ACM Press}},
  address = {{Firenze, Italy}},
  doi = {10.1145/1873951.1874248},
  isbn = {978-1-60558-933-6},
  langid = {english},
  file = {/home/nilomr/Zotero/storage/5YUDXKAI/Cannam et al. - 2010 - Sonic visualiser an open source application for v.pdf}
}

@article{coffey2019,
  title = {{{DeepSqueak}}: A Deep Learning-Based System for Detection and Analysis of Ultrasonic Vocalizations},
  shorttitle = {{{DeepSqueak}}},
  author = {Coffey, Kevin R. and Marx, Russell G. and Neumaier, John F.},
  year = {2019},
  month = apr,
  journal = {Neuropsychopharmacology},
  volume = {44},
  number = {5},
  pages = {859--868},
  publisher = {{Nature Publishing Group}},
  issn = {1740-634X},
  doi = {10.1038/s41386-018-0303-6},
  abstract = {Rodents engage in social communication through a rich repertoire of ultrasonic vocalizations (USVs). Recording and analysis of USVs has broad utility during diverse behavioral tests and can be performed noninvasively in almost any rodent behavioral model to provide rich insights into the emotional state and motor function of the test animal. Despite strong evidence that USVs serve an array of communicative functions, technical and financial limitations have been barriers for most laboratories to adopt vocalization analysis. Recently, deep learning has revolutionized the field of machine hearing and vision, by allowing computers to perform human-like activities including seeing, listening, and speaking. Such systems are constructed from biomimetic, ``deep'', artificial neural networks. Here, we present DeepSqueak, a USV detection and analysis software suite that can perform human quality USV detection and classification automatically, rapidly, and reliably using cutting-edge regional convolutional neural network architecture (Faster-RCNN). DeepSqueak was engineered to allow non-experts easy entry into USV detection and analysis yet is flexible and adaptable with a graphical user interface and offers access to numerous input and analysis features. Compared to other modern programs and manual analysis, DeepSqueak was able to reduce false positives, increase detection recall, dramatically reduce analysis time, optimize automatic syllable classification, and perform automatic syntax analysis on arbitrarily large numbers of syllables, all while maintaining manual selection review and supervised classification. DeepSqueak allows USV recording and analysis to be added easily to existing rodent behavioral procedures, hopefully revealing a wide range of innate responses to provide another dimension of insights into behavior when combined with conventional outcome measures.},
  copyright = {2019 American College of Neuropsychopharmacology},
  langid = {english},
  keywords = {Emotion,Motivation},
  file = {/home/nilomr/Zotero/storage/LSDLESAL/Coffey et al. - 2019 - DeepSqueak a deep learning-based system for detec.pdf;/home/nilomr/Zotero/storage/PVNNJH96/s41386-018-0303-6.html}
}

@misc{cohen2020,
  title = {{{TweetyNet}}: {{A}} Neural Network That Enables High-Throughput, Automated Annotation of Birdsong},
  shorttitle = {{{TweetyNet}}},
  author = {Cohen, Yarden and Nicholson, David and Sanchioni, Alexa and Mallaber, Emily K. and Skidanova, Viktoriya and Gardner, Timothy J.},
  year = {2020},
  month = oct,
  pages = {2020.08.28.272088},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.08.28.272088},
  abstract = {Songbirds have long been studied as a model system of sensory-motor learning. Many analyses of birdsong require time-consuming manual annotation of the individual elements of song, known as syllables or notes. Here we describe the first automated algorithm for birdsong annotation that is applicable to complex song such as canary song. We developed a neural network architecture, ``TweetyNet'', that is trained with a small amount of hand-labeled data using supervised learning methods. We first show TweetyNet achieves significantly lower error on Bengalese finch song than a similar method, using less training data, and maintains low error rates across days. Applied to canary song, TweetyNet achieves fully automated annotation of canary song, accurately capturing the complex statistical structure previously discovered in a manually annotated dataset. We conclude that TweetyNet will make it possible to ask a wide range of new questions focused on complex songs where manual annotation was impractical.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/home/nilomr/Zotero/storage/QK3K9XNL/Cohen et al. - 2020 - TweetyNet A neural network that enables high-thro.pdf;/home/nilomr/Zotero/storage/TRHDMPII/2020.08.28.html}
}

@article{cohen2022,
  title = {Automated Annotation of Birdsong with a Neural Network That Segments Spectrograms},
  author = {Cohen, Yarden and Nicholson, David Aaron and Sanchioni, Alexa and Mallaber, Emily K and Skidanova, Viktoriya and Gardner, Timothy J},
  editor = {Goldberg, Jesse H and Calabrese, Ronald L and Goldberg, Jesse H and Brainard, Michael},
  year = {2022},
  month = jan,
  journal = {eLife},
  volume = {11},
  pages = {e63853},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.63853},
  abstract = {Songbirds provide a powerful model system for studying sensory-motor learning. However, many analyses of birdsong require time-consuming, manual annotation of its elements, called syllables. Automated methods for annotation have been proposed, but these methods assume that audio can be cleanly segmented into syllables, or they require carefully tuning multiple statistical models. Here, we present TweetyNet: a single neural network model that learns how to segment spectrograms of birdsong into annotated syllables. We show that TweetyNet mitigates limitations of methods that rely on segmented audio. We also show that TweetyNet performs well across multiple individuals from two species of songbirds, Bengalese finches and canaries. Lastly, we demonstrate that using TweetyNet we can accurately annotate very large datasets containing multiple days of song, and that these predicted annotations replicate key findings from behavioral studies. In addition, we provide open-source software to assist other researchers, and a large dataset of annotated canary song that can serve as a benchmark. We conclude that TweetyNet makes it possible to address a wide range of new questions about birdsong.},
  keywords = {annotation,automated annotation,bengalese finches,canaries,dnn,machine learning algorithms,neural network,song syntax,songbirds,sound event detection,tweetynet},
  file = {/home/nilomr/Zotero/storage/3LVIM58J/Cohen et al. - 2022 - Automated annotation of birdsong with a neural net.pdf;/home/nilomr/Zotero/storage/NYXJT84Q/Cohen et al. - 2022 - Automated annotation of birdsong with a neural net.pdf}
}

@article{derryberry2020,
  title = {Singing in a Silent Spring: {{Birds}} Respond to a Half-Century Soundscape Reversion during the {{COVID-19}} Shutdown},
  shorttitle = {Singing in a Silent Spring},
  author = {Derryberry, Elizabeth P. and Phillips, Jennifer N. and Derryberry, Graham E. and Blum, Michael J. and Luther, David},
  year = {2020},
  month = oct,
  journal = {Science},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.abd5777},
  abstract = {Reductions in noise pollution during the pandemic shutdown allowed for more effective song production in white-crowned sparrows.},
  copyright = {Copyright \textcopyright{} 2020 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works},
  langid = {english},
  file = {/home/nilomr/Zotero/storage/T4DPB8K7/Derryberry et al. - 2020 - Singing in a silent spring Birds respond to a hal.pdf;/home/nilomr/Zotero/storage/XJ9JJ34N/science.html}
}

@article{goffinet2021,
  title = {Low-Dimensional Learned Feature Spaces Quantify Individual and Group Differences in Vocal Repertoires},
  author = {Goffinet, Jack and Brudner, Samuel and Mooney, Richard and Pearson, John},
  editor = {Goldberg, Jesse H and Behrens, Timothy E and Goldberg, Jesse H and Tchernichovski, Ofer and Linderman, Scott W},
  year = {2021},
  month = may,
  journal = {eLife},
  volume = {10},
  pages = {e67855},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.67855},
  abstract = {Increases in the scale and complexity of behavioral data pose an increasing challenge for data analysis. A common strategy involves replacing entire behaviors with small numbers of handpicked, domain-specific features, but this approach suffers from several crucial limitations. For example, handpicked features may miss important dimensions of variability, and correlations among them complicate statistical testing. Here, by contrast, we apply the variational autoencoder (VAE), an unsupervised learning method, to learn features directly from data and quantify the vocal behavior of two model species: the laboratory mouse and the zebra finch. The VAE converges on a parsimonious representation that outperforms handpicked features on a variety of common analysis tasks, enables the measurement of moment-by-moment vocal variability on the timescale of tens of milliseconds in the zebra finch, provides strong evidence that mouse ultrasonic vocalizations do not cluster as is commonly believed, and captures the similarity of tutor and pupil birdsong with qualitatively higher fidelity than previous approaches. In all, we demonstrate the utility of modern unsupervised learning approaches to the quantification of complex and high-dimensional vocal behavior.},
  keywords = {autoencoder,statistics,zebra finch},
  file = {/home/nilomr/Zotero/storage/PHKEJM8X/Goffinet et al. - 2021 - Low-dimensional learned feature spaces quantify in.pdf}
}

@misc{he2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1512.03385},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,residual networks,resnet,resnet50},
  file = {/home/nilomr/Zotero/storage/EIBIRB6Q/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/home/nilomr/Zotero/storage/CF3DF56Z/1512.html}
}

@article{hill2019,
  title = {{{AudioMoth}}: {{A}} Low-Cost Acoustic Device for Monitoring Biodiversity and the Environment},
  author = {Hill, Andrew P. and Prince, Peter and Snaddon, Jake L. and Doncaster, C. Patrick and Rogers, Alex},
  year = {2019},
  journal = {HardwareX},
  volume = {6},
  pages = {e00073},
  publisher = {{The Authors}},
  issn = {24680672},
  doi = {10.1016/j.ohx.2019.e00073},
  abstract = {Environmental sound is a powerful data source for investigating ecosystem health. To capture it, scientists commonly use ruggedized, but expensive acoustic monitoring equipment. In this paper we fully describe the hardware build of a low-cost, small, full-spectrum alternative, called AudioMoth. The credit-card sized device consists of a printed circuit board, micro-controller and a micro-electro-mechanical systems microphone. This simple to construct device facilitates: (1) deployments in remote locations, with a small size and a simple mechanism that allows it to be retrofitted into numerous low-cost ruggedized enclosures; (2) long-term monitoring, with low-power operation; (3) modular expansion, with easy to access general purpose input and output pins; and (4) acoustic detection, with onboard processing power.},
  keywords = {Acoustic monitoring,Audible and ultrasonic capability,Conservation technology,Soundscape monitoring,Wildlife monitoring},
  file = {/home/nilomr/Zotero/storage/IFDSFYXG/1-s2.0-S2468067219300306-main.pdf}
}

@article{kahl2021,
  title = {{{BirdNET}}: {{A}} Deep Learning Solution for Avian Diversity Monitoring},
  shorttitle = {{{BirdNET}}},
  author = {Kahl, Stefan and Wood, Connor M. and Eibl, Maximilian and Klinck, Holger},
  year = {2021},
  month = mar,
  journal = {Ecological Informatics},
  volume = {61},
  pages = {101236},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2021.101236},
  abstract = {Variation in avian diversity in space and time is commonly used as a metric to assess environmental changes. Conventionally, such data were collected by expert observers, but passively collected acoustic data is rapidly emerging as an alternative survey technique. However, efficiently extracting accurate species richness data from large audio datasets has proven challenging. Recent advances in deep artificial neural networks (DNNs) have transformed the field of machine learning, frequently outperforming traditional signal processing techniques in the domain of acoustic event detection and classification. We developed a DNN, called BirdNET, capable of identifying 984 North American and European bird species by sound. Our task-specific model architecture was derived from the family of residual networks (ResNets), consisted of 157 layers with more than 27 million parameters, and was trained using extensive data pre-processing, augmentation, and mixup. We tested the model against three independent datasets: (a) 22,960 single-species recordings; (b) 286~h of fully annotated soundscape data collected by an array of autonomous recording units in a design analogous to what researchers might use to measure avian diversity in a field setting; and (c) 33,670~h of soundscape data from a single high-quality omnidirectional microphone deployed near four eBird hotspots frequented by expert birders. We found that domain-specific data augmentation is key to build models that are robust against high ambient noise levels and can cope with overlapping vocalizations. Task-specific model designs and training regimes for audio event recognition perform on-par with very complex architectures used in other domains (e.g., object detection in images). We also found that high temporal resolution of input spectrograms (short FFT window length) improves the classification performance for bird sounds. In summary, BirdNET achieved a mean average precision of 0.791 for single-species recordings, a F0.5 score of 0.414 for annotated soundscapes, and an average correlation of 0.251 with hotspot observation across 121 species and 4~years of audio data. By enabling the efficient extraction of the vocalizations of many hundreds of bird species from potentially vast amounts of audio data, BirdNET and similar tools have the potential to add tremendous value to existing and future passively collected audio datasets and may transform the field of avian ecology and conservation.},
  langid = {english},
  keywords = {Avian diversity,Bioacoustics,Bird sound recognition,Conservation,Convolutional neural networks,Deep learning,Passive acoustic monitoring},
  file = {/home/nilomr/Zotero/storage/4XQRUCMY/Kahl et al. - 2021 - BirdNET A deep learning solution for avian divers.pdf;/home/nilomr/Zotero/storage/Q39D9UZX/S1574954121000273.html}
}

@article{kollmorgen2020,
  title = {Nearest Neighbours Reveal Fast and Slow Components of Motor Learning},
  author = {Kollmorgen, Sepp and Hahnloser, Richard H. R. and Mante, Valerio},
  year = {2020},
  month = jan,
  journal = {Nature},
  volume = {577},
  number = {7791},
  pages = {526--530},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1892-x},
  abstract = {Changes in behaviour resulting from environmental influences, development and learning1\textendash 5 are commonly quantified on the basis of a few hand-picked features2\textendash 4,6,7 (for example, the average pitch of acoustic vocalizations3), assuming discrete classes of behaviours (such as distinct vocal syllables)2,3,8\textendash 10. However, such methods generalize poorly across different behaviours and model systems and may miss important components of change. Here we present a more-general account of behavioural change that is based on nearest-neighbour statistics11\textendash 13, and apply it to song development in a songbird, the zebra finch3. First, we introduce the concept of `repertoire dating', whereby each rendition of a behaviour (for example, each vocalization) is assigned a repertoire time, reflecting when similar renditions were typical in the behavioural repertoire. Repertoire time isolates the components of vocal variability that are congruent with long-term changes due to vocal learning and development, and stratifies the behavioural repertoire into `regressions', `anticipations' and `typical renditions'. Second, we obtain a holistic, yet low-dimensional, description of vocal change in terms of a stratified `behavioural trajectory', revealing numerous previously unrecognized components of behavioural change on fast and slow timescales, as well as distinct patterns of overnight consolidation1,2,4,14,15~across the behavioral repertoire. We find that diurnal changes in regressions undergo only weak consolidation, whereas anticipations and typical renditions consolidate fully. Because of its generality, our nonparametric description of how behaviour evolves relative to itself\textemdash rather than to a potentially arbitrary, experimenter-defined goal2,3,14,16\textemdash appears well suited for comparing learning and change across behaviours and species17,18, as well as biological and artificial systems5.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Birdsong,Consolidation,Learning algorithms},
  file = {/home/nilomr/Zotero/storage/IHAQ5SAP/Kollmorgen et al. - 2020 - Nearest neighbours reveal fast and slow components.pdf;/home/nilomr/Zotero/storage/HGHHCFNK/s41586-019-1892-x.html}
}

@misc{lachlan2016a,
  title = {Luscinia: A Bioacoustics Analysis Computer Program},
  author = {Lachlan, Robert F},
  year = {2016},
  month = oct
}

@article{lachlan2018,
  title = {Cultural Conformity Generates Extremely Stable Traditions in Bird Song},
  author = {Lachlan, Robert F. and Ratmann, Oliver and Nowicki, Stephen},
  year = {2018},
  month = dec,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {2417},
  publisher = {{Nature Publishing Group}},
  issn = {20411723},
  doi = {10.1038/s41467-018-04728-1},
  abstract = {Cultural traditions have been observed in a wide variety of animal species. It remains unclear, however, what is required for social learning to give rise to stable traditions: what level of precision and what learning strategies are required. We address these questions by fitting models of cultural evolution to learned bird song. We recorded 615 swamp sparrow (Melospiza georgiana) song repertoires, and compared syllable frequency distributions to the output of individual-based simulations. We find that syllables are learned with an estimated error rate of 1.85\% and with a conformist bias in learning. This bias is consistent with a simple mechanism of overproduction and selective attrition. Finally, we estimate that syllable types could frequently persist for more than 500 years. Our results demonstrate conformist bias in natural animal behaviour and show that this, along with moderately precise learning, may support traditions whose stability rivals those of humans.},
  pmid = {29925831},
  keywords = {Animal behaviour,Cultural evolution},
  file = {/home/nilomr/Zotero/storage/X7TU7UAJ/full-text.pdf}
}

@inproceedings{lam2015,
  title = {Numba: A {{LLVM-based Python JIT}} Compiler},
  shorttitle = {Numba},
  booktitle = {Proceedings of the {{Second Workshop}} on the {{LLVM Compiler Infrastructure}} in {{HPC}}},
  author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
  year = {2015},
  month = nov,
  series = {{{LLVM}} '15},
  pages = {1--6},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2833157.2833162},
  abstract = {Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is often a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addition, we share our experience in building a JIT compiler using LLVM[1].},
  isbn = {978-1-4503-4005-2},
  keywords = {compiler,LLVM,Python},
  file = {/home/nilomr/Zotero/storage/8LHYNC2C/Lam et al. - 2015 - Numba a LLVM-based Python JIT compiler.pdf}
}

@inproceedings{mcfee2015,
  title = {Librosa: {{Audio}} and {{Music Signal Analysis}} in {{Python}}},
  shorttitle = {Librosa},
  booktitle = {Python in {{Science Conference}}},
  author = {McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
  year = {2015},
  pages = {18--24},
  address = {{Austin, Texas}},
  doi = {10.25080/Majora-7b98e3ed-003},
  abstract = {This document describes version 0.4.0 of librosa: a Python package for audio and music signal processing. At a high level, librosa provides implementations of a variety of common functions used throughout the field of music information retrieval. In this document, a brief overview of the library's functionality is provided, along with explanations of the design goals, software development practices, and notational conventions.},
  langid = {english},
  file = {/home/nilomr/Zotero/storage/JZBE6S9E/McFee et al. - 2015 - librosa Audio and Music Signal Analysis in Python.pdf}
}

@inproceedings{merinorecalde_eseb_2022,
  title = {Cultural Evolution in the Wild: Tracking the Landscape of Diversity in Bird Song},
  booktitle = {Congress of the {{European Society}} for {{Evolutionary Biology}}},
  author = {Merino Recalde, Nilo},
  year = {2022},
  month = aug,
  address = {{Prague, Czech Republic}}
}

@misc{merinorecalde2023,
  title = {Pykanto: V0.1.2-Beta},
  shorttitle = {Pykanto},
  author = {Merino Recalde, Nilo},
  year = {2023},
  month = feb,
  doi = {10.5281/zenodo.7603247},
  abstract = {A python library for animal vocalisation analysis},
  howpublished = {Zenodo},
  file = {/home/nilomr/Zotero/storage/42GIJAJ9/hx.html}
}

@article{morfi2021,
  title = {Deep Perceptual Embeddings for Unlabelled Animal Sound Events},
  author = {Morfi, Veronica and Lachlan, Robert F. and Stowell, Dan},
  year = {2021},
  month = jul,
  journal = {The Journal of the Acoustical Society of America},
  volume = {150},
  number = {1},
  pages = {2--11},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/10.0005475},
  abstract = {Evaluating sound similarity is a fundamental building block in acoustic perception and computational analysis. Traditional data-driven analyses of perceptual similarity are based on heuristics or simplified linear models, and are thus limited. Deep learning embeddings, often using triplet networks, have been useful in many fields. However, such networks are usually trained using large class-labelled datasets. Such labels are not always feasible to acquire. We explore data-driven neural embeddings for sound event representation when class labels are absent, instead utilising proxies of perceptual similarity judgements. Ultimately, our target is to create a perceptual embedding space that reflects animals' perception of sound. We create deep perceptual embeddings for bird sounds using triplet models. In order to deal with the challenging nature of triplet loss training with the lack of class-labelled data, we utilise multidimensional scaling (MDS) pretraining, attention pooling, and a triplet mining scheme. We also evaluate the advantage of triplet learning compared to learning a neural embedding from a model trained on MDS alone. Using computational proxies of similarity judgements, we demonstrate the feasibility of the method to develop perceptual models for a wide range of data based on behavioural judgements, helping us understand how animals perceive sounds.},
  file = {/home/nilomr/Zotero/storage/G2Z6RMVG/Morfi et al. - 2021 - Deep perceptual embeddings for unlabelled animal s.pdf}
}

@inproceedings{moritz2018,
  title = {Ray: A Distributed Framework for Emerging {{AI}} Applications},
  shorttitle = {Ray},
  booktitle = {Proceedings of the 13th {{USENIX}} Conference on {{Operating Systems Design}} and {{Implementation}}},
  author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
  year = {2018},
  month = oct,
  series = {{{OSDI}}'18},
  pages = {561--577},
  publisher = {{USENIX Association}},
  address = {{USA}},
  abstract = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray--a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.},
  isbn = {978-1-931971-47-8}
}

@article{nolet2021,
  title = {Bringing {{UMAP Closer}} to the {{Speed}} of {{Light}} with {{GPU Acceleration}}},
  author = {Nolet, Corey J. and Lafargue, Victor and Raff, Edward and Nanditale, Thejaswi and Oates, Tim and Zedlewski, John and Patterson, Joshua},
  year = {2021},
  month = mar,
  journal = {arXiv:2008.00325 [cs, stat]},
  eprint = {2008.00325},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The Uniform Manifold Approximation and Projection (UMAP) algorithm has become widely popular for its ease of use, quality of results, and support for exploratory, unsupervised, supervised, and semi-supervised learning. While many algorithms can be ported to a GPU in a simple and direct fashion, such efforts have resulted in inefficient and inaccurate versions of UMAP. We show a number of techniques that can be used to make a faster and more faithful GPU version of UMAP, and obtain speedups of up to 100x in practice. Many of these design choices/lessons are general purpose and may inform the conversion of other graph and manifold learning algorithms to use GPUs. Our implementation has been made publicly available as part of the open source RAPIDS cuML library (https://github.com/rapidsai/cuml).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,cuML,Statistics - Machine Learning,UMAP},
  file = {/home/nilomr/Zotero/storage/VM5BQQ8V/Nolet et al. - 2021 - Bringing UMAP Closer to the Speed of Light with GP.pdf;/home/nilomr/Zotero/storage/JFR8FUCQ/2008.html}
}

@article{numpy2020,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and {van der Walt}, St{\'e}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and {van Kerkwijk}, Marten H. and Brett, Matthew and Haldane, Allan and {del R{\'i}o}, Jaime Fern{\'a}ndez and Wiebe, Mark and Peterson, Pearu and {G{\'e}rard-Marchant}, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  year = {2020},
  month = sep,
  journal = {Nature},
  volume = {585},
  number = {7825},
  pages = {357--362},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1038/s41586-020-2649-2}
}

@misc{pandas2023,
  title = {Pandas-Dev/Pandas: {{Pandas}}},
  author = {{The pandas development team}},
  year = {2023},
  month = jan,
  doi = {10.5281/zenodo.7549438},
  howpublished = {Zenodo}
}

@article{priyadarshani2018,
  title = {Automated Birdsong Recognition in Complex Acoustic Environments: A Review},
  shorttitle = {Automated Birdsong Recognition in Complex Acoustic Environments},
  author = {Priyadarshani, Nirosha and Marsland, Stephen and Castro, Isabel},
  year = {2018},
  journal = {Journal of Avian Biology},
  volume = {49},
  number = {5},
  pages = {jav-01447},
  issn = {1600-048X},
  doi = {10.1111/jav.01447},
  abstract = {Conservationists are increasingly using autonomous acoustic recorders to determine the presence/absence and the abundance of bird species. Unlike humans, these recorders can be left in the field for extensive periods of time in any habitat. Although data acquisition is automated, manual processing of recordings is labour intensive, tedious, and prone to bias due to observer variations. Hence automated birdsong recognition is an efficient alternative. However, only few ecologists and conservationists utilise the existing birdsong recognisers to process unattended field recordings because the software calibration time is exceptionally high and requires considerable knowledge in signal processing and underlying systems, making the tools less user-friendly. Even allowing for these difficulties, getting accurate results is exceedingly hard. In this review we examine the state-of-the-art, summarising and discussing the methods currently available for each of the essential parts of a birdsong recogniser, and also available software. The key reasons behind poor automated recognition are that field recordings are very noisy, calls from birds that are a long way from the recorder can be faint or corrupted, and there are overlapping calls from many different birds. In addition, there can be large numbers of different species calling in one recording, and therefore the method has to scale to large numbers of species, or at least avoid misclassifying another species as one of particular interest. We found that these areas of importance, particularly the question of noise reduction, are amongst the least researched. In cases where accurate recognition of individual species is essential, such as in conservation work, we suggest that specialised (species-specific) methods of passive acoustic monitoring are required. We also believe that it is important that comparable measures, and datasets, are used to enable methods to be compared.},
  langid = {english},
  keywords = {automated recognition,birdsong recognition,birdsong recording,machine learning,noise,passive acoustic monitoring},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jav.01447},
  file = {/home/nilomr/Zotero/storage/GCZC9MUV/Priyadarshani et al. - 2018 - Automated birdsong recognition in complex acoustic.pdf;/home/nilomr/Zotero/storage/TJV8A28K/jav.html}
}

@misc{psutil2023,
  title = {Psutil},
  author = {Rodola, Giampaolo},
  year = {2023}
}

@incollection{pytorch2019,
  title = {{{PyTorch}}: {{An}} Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {8024--8035},
  publisher = {{Curran Associates, Inc.}}
}

@article{raschka2020,
  title = {Machine {{Learning}} in {{Python}}: {{Main}} Developments and Technology Trends in Data Science, Machine Learning, and Artificial Intelligence},
  author = {Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},
  year = {2020},
  journal = {arXiv preprint arXiv:2002.04803},
  eprint = {2002.04803},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{raven2019,
  title = {Raven {{Pro}}: {{Interactive Sound Analysis Software}}},
  author = {{K. Lisa Yang Center for Conservation Bioacoustics}},
  year = {2019},
  address = {{Ithaca, NY}},
  howpublished = {The Cornell Lab of Ornithology}
}

@misc{rcoreteam2021,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {{R Core Team}},
  year = {2021},
  address = {{Vienna, Austria}},
  howpublished = {R Foundation for Statistical Computing}
}

@article{sainburg2020,
  title = {Finding, Visualizing, and Quantifying Latent Structure across Diverse Animal Vocal Repertoires},
  author = {Sainburg, Tim and Thielk, Marvin and Gentner, Timothy Q.},
  year = {2020},
  month = oct,
  journal = {PLOS Computational Biology},
  volume = {16},
  number = {10},
  pages = {e1008228},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008228},
  abstract = {Animals produce vocalizations that range in complexity from a single repeated call to hundreds of unique vocal elements patterned in sequences unfolding over hours. Characterizing complex vocalizations can require considerable effort and a deep intuition about each species' vocal behavior. Even with a great deal of experience, human characterizations of animal communication can be affected by human perceptual biases. We present a set of computational methods for projecting animal vocalizations into low dimensional latent representational spaces that are directly learned from the spectrograms of vocal signals. We apply these methods to diverse datasets from over 20 species, including humans, bats, songbirds, mice, cetaceans, and nonhuman primates. Latent projections uncover complex features of data in visually intuitive and quantifiable ways, enabling high-powered comparative analyses of vocal acoustics. We introduce methods for analyzing vocalizations as both discrete sequences and as continuous latent variables. Each method can be used to disentangle complex spectro-temporal structure and observe long-timescale organization in communication.},
  langid = {english},
  keywords = {Animal communication,Bioacoustics,Birds,Finches,Hidden Markov models,Speech,Syllables,Vocalization},
  file = {/home/nilomr/Zotero/storage/TDEQQXVW/Sainburg et al. - 2020 - Finding, visualizing, and quantifying latent struc.pdf;/home/nilomr/Zotero/storage/FU3B79HM/article.html}
}

@misc{schlawack2019,
  title = {Attrs},
  author = {Schlawack, Hynek},
  year = {2019}
}

@article{scikitimage2014,
  title = {Scikit-Image: Image Processing in {{Python}}},
  author = {{van der Walt}, St{\'e}fan and Sch{\"o}nberger, Johannes L. and {Nunez-Iglesias}, Juan and Boulogne, Fran{\c c}ois and Warner, Joshua D. and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony and scikit-image {contributors}, the},
  year = {2014},
  month = jun,
  journal = {PeerJ},
  volume = {2},
  pages = {e453},
  issn = {2167-8359},
  doi = {10.7717/peerj.453},
  keywords = {Education,Image processing,Open source,Python,Reproducible research,Scientific programming,Visualization}
}

@article{scipy2020,
  title = {{{SciPy}} 1.0: {{Fundamental}} Algorithms for Scientific Computing in Python},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  year = {2020},
  journal = {Nature Methods},
  volume = {17},
  pages = {261--272},
  doi = {10.1038/s41592-019-0686-2},
  adsurl = {https://rdcu.be/b08Wh}
}

@article{searfoss2020a,
  title = {Chipper: {{Open-source}} Software for Semi-Automated Segmentation and Analysis of Birdsong and Other Natural Sounds},
  author = {Searfoss, Abigail M. and Pino, James C. and Creanza, Nicole},
  year = {2020},
  month = oct,
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {4},
  pages = {524--531},
  publisher = {{Cold Spring Harbor Laboratory}},
  issn = {2041210X},
  doi = {10.1111/2041-210X.13368},
  abstract = {Audio recording devices have changed significantly over the last 50~years, making large datasets of recordings of natural sounds, such as birdsong, easier to obtain. This increase in digital recordings necessitates an increase in high-throughput methods of analysis for researchers. Specifically, there is a need in the community for open-source methods that are tailored to recordings of varying qualities and from multiple species collected in nature. We developed Chipper, a Python-based software to semi-automate both the segmentation of acoustic signals and the subsequent analysis of their frequencies and durations. For avian recordings, we provide widgets to best determine appropriate thresholds for noise and syllable similarity, which aid in calculating note measurements and determining song syntax. In addition, we generated a set of synthetic songs with various levels of background noise to test Chipper's accuracy, repeatability and reproducibility. Chipper provides an effective way to quickly generate quantitative, reproducible measures of birdsong. The cross-platform graphical user interface allows the user to adjust parameters and visualize the resulting spectrogram and signal segmentation, providing a simplified method for analysing field recordings. Chipper streamlines the processing of audio recordings with multiple user-friendly tools and is optimized for multiple species and varying recording qualities. Ultimately, Chipper supports the use of citizen-science data and increases the feasibility of large-scale multi-species birdsong studies.},
  keywords = {acoustic signals,birdsong,citizen science,Python,recording,segmentation,software,syntax},
  file = {/home/nilomr/Zotero/storage/5HRI84EM/Searfoss et al. - 2020 - Chipper Open‐source software for semi‐automated s.pdf;/home/nilomr/Zotero/storage/YKXARI84/full-text.pdf}
}

@article{specht2002,
  title = {Avisoft-{{SASLab Pro}}},
  author = {Specht, Raimund},
  year = {2002},
  journal = {Avisoft Bioacoustics, Berlin},
  volume = {2002},
  pages = {1--723}
}

@article{steinfath2021,
  title = {Fast and Accurate Annotation of Acoustic Signals with Deep Neural Networks},
  author = {Steinfath, Elsa and {Palacios-Mu{\~n}oz}, Adrian and Rottsch{\"a}fer, Julian R and Yuezak, Deniz and Clemens, Jan},
  editor = {Calabrese, Ronald L and Egnor, SE Roian and Troyer, Todd},
  year = {2021},
  month = nov,
  journal = {eLife},
  volume = {10},
  pages = {e68837},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.68837},
  abstract = {Acoustic signals serve communication within and across species throughout the animal kingdom. Studying the genetics, evolution, and neurobiology of acoustic communication requires annotating acoustic signals: segmenting and identifying individual acoustic elements like syllables or sound pulses. To be useful, annotations need to be accurate, robust to noise, and fast. We here introduce DeepAudioSegmenter (DAS), a method that annotates acoustic signals across species based on a deep-learning derived hierarchical presentation of sound. We demonstrate the accuracy, robustness, and speed of DAS using acoustic signals with diverse characteristics from insects, birds, and mammals. DAS comes with a graphical user interface for annotating song, training the network, and for generating and proofreading annotations. The method can be trained to annotate signals from new species with little manual annotation and can be combined with unsupervised methods to discover novel signal types. DAS annotates song with high throughput and low latency for experimental interventions in realtime. Overall, DAS is a universal, versatile, and accessible tool for annotating acoustic communication signals.},
  keywords = {acoustic communication,annotation,bird,deep learning,fly,song},
  file = {/home/nilomr/Zotero/storage/TU2LEESY/Steinfath et al. - 2021 - Fast and accurate annotation of acoustic signals w.pdf}
}

@article{stowell2014,
  title = {Automatic Large-Scale Classification of Bird Sounds Is Strongly Improved by Unsupervised Feature Learning},
  author = {Stowell, Dan and Plumbley, Mark D.},
  year = {2014},
  month = jul,
  journal = {PeerJ},
  volume = {2},
  pages = {e488},
  issn = {2167-8359},
  doi = {10.7717/peerj.488},
  abstract = {Automatic species classification of birds from their sound is a computational tool of increasing importance in ecology, conservation monitoring and vocal communication studies. To make classification useful in practice, it is crucial to improve its accuracy while ensuring that it can run at big data scales. Many approaches use acoustic measures based on spectrogram-type data, such as the Mel-frequency cepstral coefficient (MFCC) features which represent a manually-designed summary of spectral information. However, recent work in machine learning has demonstrated that features learnt automatically from data can often outperform manually-designed feature transforms. Feature learning can be performed at large scale and ``unsupervised'', meaning it requires no manual data labelling, yet it can improve performance on ``supervised'' tasks such as classification. In this work we introduce a technique for feature learning from large volumes of bird sound recordings, inspired by techniques that have proven useful in other domains. We experimentally compare twelve different feature representations derived from the Mel spectrum (of which six use this technique), using four large and diverse databases of bird vocalisations, classified using a random forest classifier. We demonstrate that in our classification tasks, MFCCs can often lead to worse performance than the raw Mel spectral data from which they are derived. Conversely, we demonstrate that unsupervised feature learning provides a substantial boost over MFCCs and Mel spectra without adding computational complexity after the model has been trained. The boost is particularly notable for single-label classification tasks at large scale. The spectro-temporal activations learned through our procedure resemble spectro-temporal receptive fields calculated from avian primary auditory forebrain. However, for one of our datasets, which contains substantial audio data but few annotations, increased performance is not discernible. We study the interaction between dataset characteristics and choice of feature representation through further empirical analysis.},
  pmcid = {PMC4106198},
  pmid = {25083350},
  file = {/home/nilomr/Zotero/storage/CTFMQ9TD/Stowell and Plumbley - 2014 - Automatic large-scale classification of bird sound.pdf}
}

@article{stowell2019,
  title = {Automatic Acoustic Detection of Birds through Deep Learning: {{The}} First {{Bird Audio Detection}} Challenge},
  shorttitle = {Automatic Acoustic Detection of Birds through Deep Learning},
  author = {Stowell, Dan and Wood, Michael D. and Pamu{\l}a, Hanna and Stylianou, Yannis and Glotin, Herv{\'e}},
  year = {2019},
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {3},
  pages = {368--380},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13103},
  abstract = {Assessing the presence and abundance of birds is important for monitoring specific species as well as overall ecosystem health. Many birds are most readily detected by their sounds, and thus, passive acoustic monitoring is highly appropriate. Yet acoustic monitoring is often held back by practical limitations such as the need for manual configuration, reliance on example sound libraries, low accuracy, low robustness, and limited ability to generalise to novel acoustic conditions. Here, we report outcomes from a collaborative data challenge. We present new acoustic monitoring datasets, summarise the machine learning techniques proposed by challenge teams, conduct detailed performance evaluation, and discuss how such approaches to detection can be integrated into remote monitoring projects. Multiple methods were able to attain performance of around 88\% area under the receiver operating characteristic (ROC) curve (AUC), much higher performance than previous general-purpose methods. With modern machine learning, including deep learning, general-purpose acoustic bird detection can achieve very high retrieval rates in remote monitoring data, with no manual recalibration, and no pretraining of the detector for the target species or the acoustic conditions in the target environment.},
  langid = {english},
  keywords = {bird,deep learning,machine learning,passive acoustic monitoring,sound},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13103},
  file = {/home/nilomr/Zotero/storage/U7DYZ9ZR/Stowell et al. - 2019 - Automatic acoustic detection of birds through deep.pdf;/home/nilomr/Zotero/storage/KVV4U7N4/2041-210X.html}
}

@article{stowell2021,
  title = {Computational Bioacoustics with Deep Learning: A Review and Roadmap},
  shorttitle = {Computational Bioacoustics with Deep Learning},
  author = {Stowell, Dan},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.06725 [cs, eess, q-bio]},
  eprint = {2112.06725},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, q-bio},
  abstract = {Animal vocalisations and natural soundscapes are fascinating objects of study, and contain valuable evidence about animal behaviours, populations and ecosystems. They are studied in bioacoustics and ecoacoustics, with signal processing and analysis an important component. Computational bioacoustics has accelerated in recent decades due to the growth of affordable digital sound recording devices, and to huge progress in informatics such as big data, signal processing and machine learning. Methods are inherited from the wider field of deep learning, including speech and image processing. However, the tasks, demands and data characteristics are often different from those addressed in speech or music analysis. There remain unsolved problems, and tasks for which evidence is surely present in many acoustic signals, but not yet realised. In this paper I perform a review of the state of the art in deep learning for computational bioacoustics, aiming to clarify key concepts and identify and analyse knowledge gaps. Based on this, I offer a subjective but principled roadmap for computational bioacoustics with deep learning: topics that the community should aim to address, in order to make the most of future developments in AI and informatics, and to use audio data in answering zoological and ecological questions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Quantitative Biology - Quantitative Methods},
  file = {/home/nilomr/Zotero/storage/6TDE5SBD/Stowell - 2021 - Computational bioacoustics with deep learning a r.pdf;/home/nilomr/Zotero/storage/4UUVWWNF/2112.html}
}

@article{tachibana2015,
  title = {Variability in the Temporal Parameters in the Song of the {{Bengalese}} Finch ({{Lonchura}} Striata Var. Domestica)},
  author = {Tachibana, Ryosuke O. and Koumura, Takuya and Okanoya, Kazuo},
  year = {2015},
  month = dec,
  journal = {Journal of Comparative Physiology A},
  volume = {201},
  number = {12},
  pages = {1157--1168},
  issn = {1432-1351},
  doi = {10.1007/s00359-015-1046-z},
  abstract = {Birdsong provides a unique model for studying the control mechanisms of complex sequential behaviors. The present study aimed to demonstrate that multiple factors affect temporal control in the song production. We analyzed the song of Bengalese finches in various time ranges to address factors that affected the duration of acoustic elements (notes) and silent intervals (gaps). The gaps showed more jitter across song renditions than did notes. Gaps had longer duration in branching points of song sequence than in stereotypic transitions, and the duration of a gap was correlated with the duration of the note that preceded the gap. When looking at the variation among song renditions, we found notable factors in three time ranges: within-day drift, within-bout changes, and local jitter. Note durations shortened over time from morning to evening. Within each song bout note durations lengthened as singing progressed, while gap durations lengthened only during the late part of song bout. Further analysis after removing these drift factors confirmed that the jitter remained in local song sequences. These results suggest distinct sources of temporal variability exist at multiple levels on the basis of this note\textendash gap relationship, and that song comprised a mixture of these sources.},
  langid = {english},
  keywords = {bengalese finch,drift,tamporal variability},
  file = {/home/nilomr/Zotero/storage/4WLSTJ7L/Tachibana et al. - 2015 - Variability in the temporal parameters in the song.pdf}
}

@article{taschuk2017,
  title = {Ten Simple Rules for Making Research Software More Robust},
  author = {Taschuk, Morgan and Wilson, Greg},
  year = {2017},
  month = apr,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {4},
  pages = {e1005412},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005412},
  abstract = {Software produced for research, published and otherwise, suffers from a number of common problems that make it difficult or impossible to run outside the original institution or even off the primary developer's computer. We present ten simple rules to make such software robust enough to be run by anyone, anywhere, and thereby delight your users and collaborators.},
  langid = {english},
  keywords = {Bioinformatics,code,Computer software,documentation,Open source software,Operating systems,Reproducibility,software,Software development,Software engineering,Software tools},
  file = {/home/nilomr/Zotero/storage/DNTURGZA/Taschuk and Wilson - 2017 - Ten simple rules for making research software more.pdf}
}

@article{tchernichovski2000,
  title = {A Procedure for an Automated Measurement of Song Similarity},
  author = {Tchernichovski, Ofer and Nottebohm, Fernando and Ho, Ching Elizabeth and Pesaran, Bijan and Mitra, Partha Pratim},
  year = {2000},
  month = jun,
  journal = {Animal Behaviour},
  volume = {59},
  number = {6},
  pages = {1167--1176},
  issn = {00033472},
  doi = {10.1006/anbe.1999.1416},
  langid = {english},
  file = {/home/nilomr/Zotero/storage/I9LNEFCJ/Tchernichovski et al. - 2000 - A procedure for an automated measurement of song s.pdf}
}

@misc{thomas2021,
  title = {A Practical Guide for Generating Unsupervised, Spectrogram-Based Latent Space Representations of Animal Vocalizations},
  author = {Thomas, Mara and Jensen, Frants H. and Averly, Baptiste and Demartsev, Vlad and Manser, Marta B. and Sainburg, Tim and Roch, Marie A. and {Strandburg-Peshkin}, Ariana},
  year = {2021},
  month = dec,
  pages = {2021.12.16.472881},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.12.16.472881},
  abstract = {The manual detection, analysis, and classification of animal vocalizations in acoustic recordings is laborious and requires expert knowledge. Hence, there is a need for objective, generalizable methods that detect underlying patterns in these data, categorize sounds into distinct groups, and quantify similarities between them. Among all computational methods that have been proposed to accomplish this, neighborhood-based dimensionality reduction of spectrograms to produce a latent-space representation of calls stands out for its conceptual simplicity and effectiveness. Using a dataset of manually annotated meerkat (Suricata suricatta) vocalizations, we demonstrate how this method can be used to obtain meaningful latent space representations that reflect the established taxonomy of call types. We analyze strengths and weaknesses of the proposed approach, give recommendations for its usage and show application examples, such as the classification of ambiguous calls and the detection of mislabeled calls. All analyses are accompanied by example code to help researchers realize the potential of this method for the study of animal vocalizations.},
  chapter = {Confirmatory Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/home/nilomr/Zotero/storage/DD28LDN4/Thomas et al. - 2021 - A practical guide for generating unsupervised, spe.pdf;/home/nilomr/Zotero/storage/7M3RYVK5/2021.12.16.472881v1.html}
}

@article{tqdm2019,
  title = {Tqdm: {{A}} Fast, Extensible Progress Meter for Python and {{CLI}}},
  author = {{da Costa-Luis}, Casper O.},
  year = {2019},
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {37},
  pages = {1277},
  publisher = {{The Open Journal}},
  doi = {10.21105/joss.01277}
}

@misc{ujason2023,
  title = {Ultrajson},
  author = {{van Kemenade}, Hugo and Woodsend, Br{\'e}nainn and Hamr{\'e}n, Joakim and {JustAnotherArchivist} and Crall, Jon and O'Mahony, Kieran and Lay, Eric Le and 民憙), Hong Minhee (洪 and Sychev, Mikhail and Swenson, David W.H. and Dawborn, Tim and {Garcia-Armas}, Mario and Ceccon, R{\^o}mulo and Moiron, Jason and G{\'o}rny, Micha{\l} and {NaN-git} and Naoki, Inada and Beasley, Ben and Zhou, Logan and Borisov, Mikhail and Ayd, William and Johnson, Adam and Ebrahim, Ali and Bangert, Ben and Schubert, Benjamin and Forehand, Brandon and Bystr{\"o}m, Carl and Hsiao, Chen-Han (Stanley) and {CozyDoomer}},
  year = {2023},
  month = jan,
  doi = {10.5281/zenodo.7510698},
  howpublished = {Zenodo}
}

@article{ulloa2021,
  title = {Scikit-Maad: {{An}} Open-Source and Modular Toolbox for Quantitative Soundscape Analysis in {{Python}}},
  shorttitle = {Scikit-Maad},
  author = {Ulloa, Juan Sebasti{\'a}n and Haupert, Sylvain and Latorre, Juan Felipe and Aubin, Thierry and Sueur, J{\'e}r{\^o}me},
  year = {2021},
  journal = {Methods in Ecology and Evolution},
  volume = {12},
  number = {12},
  pages = {2334--2340},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13711},
  abstract = {Passive acoustic monitoring is increasingly being applied to terrestrial, marine and freshwater environments, providing cost-efficient methods for surveying biodiversity. However, processing the avalanche of audio recordings remains challenging, and represents nowadays a major bottleneck that slows down its application in research and conservation. We present scikit-maad, an open-source Python package dedicated to the analysis of environmental audio recordings. This package was designed to (a) load and process digital audio, (b) segment and find regions of interest, (c) compute acoustic features and (d) estimate sound pressure levels. The package also provides field recordings and a comprehensive online documentation that includes practical examples with step-by-step instructions for beginners and advanced users. scikit-maad opens the possibility to efficiently scan large audio datasets and easily integrate additional machine learning Python packages into the analysis, allowing to measure acoustic properties and identify key patterns in all kinds of soundscapes. To support reproducible research, the package is released under the BSD open-source licence, which allows unrestricted redistribution for commercial and private use. This development will create synergies between the community of ecoacousticians, such as engineers, data scientists, ecologists, biologists and conservation practitioners, to explore and understand the processes underlying the acoustic diversity of ecological systems.},
  langid = {english},
  keywords = {acoustic indices,bioacoustics,ecoacoustics,pattern recognition,sound pressure level},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13711},
  file = {/home/nilomr/Zotero/storage/7ANYQJRH/Ulloa et al. - 2021 - scikit-maad An open-source and modular toolbox fo.pdf;/home/nilomr/Zotero/storage/I2RS7FZL/2041-210X.html}
}

@article{vansegbroeck2017,
  title = {{{MUPET}}\textemdash{{Mouse Ultrasonic Profile ExTraction}}: {{A~Signal Processing Tool}} for {{Rapid}} and {{Unsupervised Analysis}} of {{Ultrasonic Vocalizations}}},
  shorttitle = {{{MUPET}}\textemdash{{Mouse Ultrasonic Profile ExTraction}}},
  author = {Van Segbroeck, Maarten and Knoll, Allison T. and Levitt, Pat and Narayanan, Shrikanth},
  year = {2017},
  month = may,
  journal = {Neuron},
  volume = {94},
  number = {3},
  pages = {465-485.e5},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2017.04.005},
  abstract = {Vocalizations play a significant role in social communication across species. Analyses in rodents have used a limited number of spectro-temporal measures to compare ultrasonic vocalizations (USVs), which limits the ability to address repertoire complexity in the context of behavioral states. Using an automated and unsupervised signal processing approach, we~report the development of MUPET (Mouse Ultrasonic Profile ExTraction) software, an open-access MATLAB tool that provides data-driven, high-throughput analyses of USVs. MUPET measures, learns, and compares syllable types and provides an automated time stamp of syllable events. Using USV data from a large mouse genetic reference panel and open-source datasets produced in different social contexts, MUPET analyzes the fine details of syllable production and repertoire use. MUPET thus serves as a new tool for USV repertoire analyses, with the capability to be adapted for use with other species.},
  langid = {english},
  keywords = {automated,communication,high-throughput,machine learning,repertoire,signal processing,social behavior,software,syllable,ultrasonic vocalizations},
  file = {/home/nilomr/Zotero/storage/7MLXAUCS/Van Segbroeck et al. - 2017 - MUPET—Mouse Ultrasonic Profile ExTraction A Signa.pdf;/home/nilomr/Zotero/storage/VRBKEDEF/S0896627317302982.html}
}

@article{youngblood2022,
  title = {Content Bias in the Cultural Evolution of House Finch Song},
  author = {Youngblood, Mason and Lahti, David C.},
  year = {2022},
  month = mar,
  journal = {Animal Behaviour},
  volume = {185},
  pages = {37--48},
  issn = {0003-3472},
  doi = {10.1016/j.anbehav.2021.12.012},
  abstract = {We used three years of house finch, Haemorhous mexicanus, song recordings spanning four decades in the introduced eastern range to assess how individual level cultural transmission mechanisms drive population level changes in birdsong. First, we developed an agent-based model (available as a new R package called `TransmissionBias') that simulates the cultural transmission of house finch song given different parameters related to transmission biases, or biases in social learning that modify the probability of adoption of particular cultural variants. Next, we used approximate Bayesian computation and machine learning to estimate what parameter values likely generated the temporal changes in diversity in our observed data. We found evidence that strong content bias, likely targeted towards syllable complexity, plays a central role in the cultural evolution of house finch song in the New York metropolitan area. Frequency and demonstrator biases appear to be neutral or absent. Additionally, we estimated that house finch song is transmitted with extremely high fidelity. Future studies can use our simulation framework to better understand how cultural transmission and population declines influence song diversity in wild populations.},
  langid = {english},
  keywords = {birdsong,content bias,cultural evolution,house finch,machine learning,social learning,transmission bias,transmission fidelity},
  file = {/home/nilomr/Zotero/storage/XX2FQTWA/Youngblood and Lahti - 2022 - Content bias in the cultural evolution of house fi.pdf;/home/nilomr/Zotero/storage/R24IQI4V/S000334722100395X.html}
}

@article{zsebok2018,
  title = {``{{Ficedula}}'': An Open-Source {{MATLAB}} Toolbox for Cutting, Segmenting and Computer-Aided Clustering of Bird Song},
  author = {Zseb{\H o}k, S{\'a}ndor and Bl{\'a}zi, Gy{\"o}rgy and Laczi, Mikl{\'o}s and Nagy, Gergely and Vaskuti, {\'E}va and Garamszegi, L{\'a}szl{\'o} Zsolt},
  year = {2018},
  journal = {Journal of Ornithology},
  doi = {10.1007/s10336-018-1581-9},
  abstract = {{$<$}h3 class="a-plus-plus"{$>$}Abstract{$<$}/h3{$>$}                   {$<$}p class="a-plus-plus"{$>$}Qualitative and quantitative assessments of bird song repertoires are important in studies related to song learning, sexual selection and cultural evolution. Despite methods for automatic analysis, it is still necessary to engage in manual cutting, segmenting and clustering of bird song elements in many cases. Here, we describe a program, the Ficedula Toolbox, which has been made available for free to the bird song research community and has recently come into extensive use. The main advantages of this package are the opportunity to conduct all processing steps in one framework and the option of carrying out computer-aided manual clustering. Output files are ready for further analyses, such as estimation of repertoire size, sequential analysis and repertoire overlap calculation. With this program, findings based on empirical data from the Collared Flycatcher ({$<$}em class="a-plus-plus"{$>$}Ficedula albicollis{$<$}/em{$>$}) song show high inter-observer similarity, and thus, reproducible results. The toolbox may be especially applicable to the analysis of song in species with moderately high repertoires.{$<$}/p{$>$}},
  file = {/home/nilomr/Zotero/storage/CLWL2UYX/Zsebők et al. - 2018 - “Ficedula” an open-source MATLAB toolbox for cutt.pdf;/home/nilomr/Zotero/storage/DHYQMA7M/Zsebők et al. - 2018 - “Ficedula” an open-source MATLAB toolbox for cutt.pdf;/home/nilomr/Zotero/storage/GSVDB6GB/Zsebők et al. - “Ficedula” an open-source MATLAB toolbox for cutt.pdf}
}
